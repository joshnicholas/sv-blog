---
title: If you’re going to change the world, you must reflect it first
date: 2019-04-04
---

<!--kg-card-begin: html--><p>I find taking public transport or hopping a plane immensely stressful. Not because of the shoddy infrastructure, waiting around, or poor service. Because I&#8217;m 6&#8243;4 with disproportionately long legs in a world built by people who aren&#8217;t.</p><br>
<p>As I <a href="https://joshnicholas.com/why-techies-think-they-can-change-the-world/" target="_blank" rel="noopener noreferrer">continue</a> to read <a href="https://www.worldcat.org/title/coders-how-software-programmers-think-and-how-their-thinking-is-changing-our-world/oclc/1078434444&#038;referer=brief_results" target="_blank" rel="noopener noreferrer">Coders</a>, I&#8217;m increasingly worried how this same phenomena will play out in a world full of algorithmic black boxes. Code so complex and systems so arcane that <a href="https://annotations.joshnicholas.com/2019/04/03/looking-forward-to.html" target="_blank" rel="noopener noreferrer">even their creators struggle to understand them</a>.</p><br>
<p>Techies love to talk about scale and putting their creations in front of millions. But for this to work they themselves need to be drawn from a representative pool.</p><br>
<p>Otherwise you get <a href="https://twitter.com/joshcnicholas/status/1102581901011968000?s=20" target="_blank" rel="noopener noreferrer">self driving cars that are more likely to hit black people</a>. Or image recognition that thinks black People are gorillas.</p><br>
<blockquote><p>&#8230;then Alciné scrolled over to a picture of himself and a friend, in a selfie they’d taken at an outdoor concert: She looms close in the view, while he’s peering, smiling, over her right shoulder. <strong>Alciné is African American, and so is his friend. And the label that Google Photos had generated? “Gorillas.” </strong>It wasn’t just that single photo, either. Over fifty snapshots of the two from that day had been identified as “gorillas.”</p><br>
</blockquote>
<p>This isn&#8217;t only a Google problem. Or even a Silicon Valley problem. There are also stories of algorithms trained in China and South Korea that have trouble recognising Caucasian faces.</p><br>
<p>As a journalist with a diverse ethnic and cultural background I had trouble understanding why my editors took so much convincing to run foreign stories. With a family spread around the globe, I could see myself in the Rohingya as much as an Australian farmer.</p><br>
<p>These issues are linked &#8211; what we value, notice and think of as &#8220;normal&#8221; are all informed by our personal stories. If you grow up or work in a monoculture, that will influence the issues you see, the solutions you propose and contingencies you plan for.</p><br>
<p>But the world isn&#8217;t a monoculture. There are 6&#8243;4 people who would like to ride the bus. There will be people who aren&#8217;t like you but need to cross the street safely, or be judged fairly.</p><br>
<p>Who will be deeply offended by racial epithets, which are themselves linked to why they aren&#8217;t represented in a database.</p><br>
<p>If you&#8217;re going to try and change the world for the better, you need to be of the world. There will always be edge cases, but without diversity they will be systemic. They will be disastrous.</p><br>
<blockquote><p>&#8230;why couldn’t Google’s AI recognize an African American face? Very likely because it hadn’t been trained on enough of them. <strong>Most data sets of photos that coders in the West use for training face-recognition are heavily white</strong>, so the neural nets easily learn to make nuanced recognitions of white people—but <strong>they only develop a hazy sense of what black people look like</strong>.</p><br>
</blockquote>
<p><em>As always my emphasis.</em></p><br>
<!--kg-card-end: html-->