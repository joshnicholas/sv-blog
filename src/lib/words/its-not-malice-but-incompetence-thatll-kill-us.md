---
title: Its not malice but incompetence thatll kill us
date: 2019-12-07
---

<!--kg-card-begin: html--><p>I&#8217;ve been reading <a href="https://www.worldcat.org/title/you-look-like-a-thing-and-i-love-you/oclc/1128058352&#038;referer=brief_results">You look like a thing and I love you</a> by Janelle Shane. And, honestly, it&#8217;s some of the best skewering of Artificial Intelligence I&#8217;ve come across. But amid the funny stories of AI incompetence &#8211; only recognising sheep when they&#8217;re in fields, thinking a goat in a tree is a giraffe etc. &#8211; there&#8217;s a serious point about the impact of these limitations.</p><br>
<blockquote><p>As more of our daily lives are governed by algorithms, the quirks of AI are beginning to have consequences far beyond the merely inconvenient. Recommendation algorithms embedded in YouTube point people toward ever more polarizing content, traveling in a few short clicks from mainstream news to videos by hate groups and conspiracy theorists&#8230;</p><br>
</blockquote>
<blockquote><p>&#8230;The algorithms that make decisions about parole, loans, and resume screening <strong>are not impartial but can be just as prejudiced as the humans theyâ€™re supposed to replaceâ€”sometimes even more so</strong>. AI-powered surveillance canâ€™t be bribed, but it also canâ€™t raise moral objections to anything itâ€™s asked to do. It can also make mistakes when itâ€™s misusedâ€”or even when itâ€™s hacked. <strong>Researchers have discovered that something as seemingly insignificant as a small sticker can make an image recognition AI think a gun is a toaster, and a low-security fingerprint reader can be fooled more than 77 percent of the time with a single master fingerprint</strong>.</p><br>
</blockquote>
<p>People are generally quick to chalk up to malice what is better explained by incompetence. The righteous outrage in the papers is full of pinstriped fat cats rather than honest mistakes and fallible processes (sometimes rightly so, but probably not as often as portrayed).</p><br>
<p>It seems our fears for the future fall into this same trap. What truly scares me about AI is generally the outcome of a perfectly running system. Skynet murderbots conquering the world. Or batallions of worker drones tilting the balance further in favour of capital and the technologically competent.</p><br>
<p>But my recent studies of machine learning, and this book specifically, make we wonder whether the true worry shouldn&#8217;t be bias and incompetence. Not a murderous bot. Rather one that just isn&#8217;t ready for prime time. In that sense the problem isn&#8217;t the technology itself, but the underlying systems that deployed it (I didn&#8217;t explicitly write &#8220;capitalism&#8221; here, but I wouldn&#8217;t fault you for reading it in ðŸ˜‡).</p><br>
<p>If there&#8217;s malice in the system, that&#8217;s where to find it.</p><br>
<blockquote><p>When people think of AI disaster, they think of AIs refusing orders, deciding that their best interests lie in killing all humans, or creating terminator bots. But <strong>all those disaster scenarios assume a level of critical thinking and a humanlike understanding of the world that AIs wonâ€™t be capable of for the foreseeable future</strong>. As leading machine learning researcher Andrew Ng put it, worrying about an AI takeover is like worrying about overcrowding on Mars.</p><br>
</blockquote>
<p>One of Shane&#8217;s principles of &#8220;AI weirdness&#8221; is that it does not really understand the problem you want it to solve. Another is that it will take the path of least resistance to achieve what you tell it to.</p><br>
<p>When you&#8217;re using AI to play a game this can lead to some obnoxious cheating. When you&#8217;re deploying it in the real world this can result in further entrenching bias, hierarchy and revealed preference. Often in ways we don&#8217;t understand and expect. Particularly if the implementors aren&#8217;t aware of the underlying bias in the system they are a part of.</p><br>
<blockquote><p>The problem with designing an AI to screen candidates for us: we arenâ€™t really asking the AI to identify the best candidates. Weâ€™re asking it to identify the candidates that most resemble the ones our human hiring managers liked in the past. That might be okay if the human hiring managers made great decisions. But most US companies have a diversity problem, particularly among managers and particularly in the way that hiring managers evaluate resumes and interview candidates. All else being equal, resumes with white-male-sounding names are more likely to get interviews than those with female-and/ or minority-sounding names. 5 Even hiring managers who are female and/ or members of a minority themselves tend to unconsciously favor white male candidates. <strong>Plenty of bad and/ or outright harmful AI programs are designed by people who thought they were designing an AI to solve a problem but were unknowingly training it to do something entirely different</strong>.</p><br>
</blockquote>
<p><em></em></p><br>
<p><em>As always my emphasis</em></p><br>
<!--kg-card-end: html-->